#! /usr/bin/env python

import sys

def main(): 
    #
    # Imports
    #
    import metadata
    import time
    from seqdata import BarcodeClusterer
    from misc import Progress
    import multiprocessing
    import sqlite3
    import os
    
    #
    # check input and get commandline args
    #
    try:
        analysisfolder = metadata.AnalysisFolder(sys.argv[1])
        analysisfolder.readindexTsv()
        if analysisfolder.settings.temp: analysisfolder.copy_to_temp()

    except IndexError: sys.stderr.write('please supply a commandline on format:\n'+os.path.basename(__file__)+' <analysis-output-folder> [reanalyze=bool] [clusterid=int]\n');sys.exit()

    #
    # check analysis folder
    #
    if not analysisfolder.checkIntegrity() == 'PASS': print analysisfolder.checkIntegrity()+'\nERROR: Now exiting'

    #
    # create a logfile
    #
    logfile = open(analysisfolder.logpath+'/'+time.strftime("%y%m%d-%H:%M:%S",time.localtime())+'_analyseClusters.log.txt','w',1)
    logfile.write('cmd: '+' '.join(sys.argv)+'\n')
    analysisfolder.logfile = logfile

    #
    # check if reanalysis flag is set and do update db if req
    #
    try:
        for arg in sys.argv:
            if arg.split('=')[0] in ['reanalyze','reAnalyze'] and eval(arg.split('=')[1])==True:
                sys.stderr.write('Reanalyzing! ie setting all clusters to be updated!\n')
                reAnalyze(analysisfolder)
    except IndexError: pass
    #
    # check if the target region bedfile is defined
    #
    if not analysisfolder.settings.targetRegionBed or not os.path.exists(analysisfolder.settings.targetRegionBed):
        msg = 'WARNING: cant find the target definition bed file!!\n'
        msg+= '         (If run is not targeted you can safely ignore this message).\n'
        sys.stderr.write(msg);
        logfile.write(msg)
    
    # just a progeress meter
    progress = Progress(analysisfolder.results.barcodeClusterCount,logfile=analysisfolder.logfile, unit='clusters',printint=1)
    updateChunk = []

    # flag for multiprocessing or not
    runInParallel = True
    if analysisfolder.settings.debug: runInParallel = False
    
    #
    # run the analysis functions for all the clusters aither using multiprocessing or not depending on the flag setting above
    #
    if runInParallel:
        poolOfProcesses = multiprocessing.Pool(int(analysisfolder.settings.parallelProcesses),maxtasksperchild=100000000)
        parallelResults = poolOfProcesses.imap_unordered(foreachCluster,cluster_generator(analysisfolder),chunksize=10)
        for cluster in parallelResults:
            updateChunk = forResultingCluster(progress,cluster,updateChunk)

    else: # serial
        for cluster in cluster_generator(analysisfolder):
            cluster = foreachCluster(cluster)
            updateChunk = forResultingCluster(progress,cluster,updateChunk)

    updateClusterChunk(cluster.analysisfolder,updateChunk) # last update to not lose end values
    
    if analysisfolder.database_in_temp: analysisfolder.copy_from_temp()
    
    logfile.write('analyseClusters FINISHED\n')

def reAnalyze(analysisfolder):
    """ this function resets the analyzed flag for all clusters in the database so that they are analyzed again """
    analysisfolder.logfile.write('reanalyzing!\n')
    analysisfolder.database.getConnection()
    analysisfolder.database.c.execute('UPDATE barcodeClusters SET analyzed=?',(False,))
    analysisfolder.database.commitAndClose()

def forResultingCluster(progress,cluster,updateChunk):
    """ post analysis processing for each cluster """
    
    #
    # add the info to the cunk of clusters to be updated
    #
    if cluster.analyzed: updateChunk.append(cluster.updatedb(doUpdate=False,returnTuple=True))
    progress.update()
    
    #
    # if the chunk is more than 1000 clusters update the database table
    #
    if len(updateChunk) >= 1000:
        updateClusterChunk(cluster.analysisfolder,updateChunk)
        updateChunk = []
    
    # pass the chuck on to the next cluster
    return updateChunk

def cluster_generator(analysisfolder):
    """ function that gets the clusterids from the database and yields seqdata.BarcodeCluster() objects """
    
    from seqdata import BarcodeCluster
    from seqdata import BarcodeClusterer
    import sys
    
    barcodeClusterer = BarcodeClusterer(analysisfolder)
    
    cluster_ids = None
    for arg in sys.argv:
        if arg.split('=')[0] in ['clusterid','clusterId','clusterID']:
            cluster_ids = [int(arg.split('=')[1])]
            cluster_ids = [ BarcodeCluster(int(cluster_id), analysisfolder) for cluster_id in cluster_ids ]
            for cluster in cluster_ids: cluster.loadClusterInfo()
    if not cluster_ids:
        #cluster_ids = barcodeClusterer.getBarcodeClusterIds(shuffle=True,byMixedClusterReadCount=True)
        analysisfolder.database.getConnection()
        cluster_ids = list(analysisfolder.database.getAllClustersLoaded(analysisfolder))
        import random
        random.shuffle(cluster_ids)
    
    # for cluster_id in cluster_ids:
    #     cluster = BarcodeCluster(int(cluster_id), analysisfolder)
    #     cluster.loadClusterInfo()
    for cluster in cluster_ids:
        yield cluster

def foreachCluster(cluster):
    """ functions to be run to analyze each of the clusters """
    
    #load info from the database
    #cluster.loadClusterInfo()
    
    # if cluster is not analyzed and meet the req. run the analysis
    if not cluster.analyzed and cluster.readPairCount >= int(cluster.analysisfolder.settings.minPairsPerCluster):
        cluster.analyze(createBamIndex=True)
        cluster.findTargetCoverage()
        if cluster.targetInfo != []: cluster.findHetroZygousBasesInTarget(include_hetro=True,include_homo_reference=False,include_homo_non_reference=True)
        cluster.removeAllFiles()
    
    # if already analyzed move on
    elif cluster.analyzed:
        cluster.analyzed = False;
        if cluster.analysisfolder.settings.debug: print cluster.id,'already analyzed'
    
    # flag cluster if below readcount cutof
    elif cluster.readPairCount < int(cluster.analysisfolder.settings.minPairsPerCluster):
        cluster.analyzed = True;
        cluster.annotations['LowReadCount']=True;
        if cluster.analysisfolder.settings.debug: print cluster.id,' low read count'
    
    return cluster

def updateClusterChunk(analysisfolder,chunk):
    """ function for updating the information for a chunk of clusters in the database """
    
    # Note that the parts with lock does not function properly due to the use of sqllite3 which only allows one conection to the database at a time

    with analysisfolder.database.lock:
        analysisfolder.database.writeInProgress.value = True
        
        #
        # imports
        #
        import sqlite3
        import time
        
        #set initial flag
        updated = False
        
        while not updated: # try to update until sucess
            try: 
                
                # get connection
                analysisfolder.database.getConnection()
                
                # check the columns in the table and add missing columns
                analysisfolder.database.c.execute('PRAGMA table_info(barcodeClusters)')
                columnNames = [col[1] for col in analysisfolder.database.c.fetchall()]
                if 'constructTypes' not in columnNames:
                    try: analysisfolder.logfile.write('Creating columns in table barcodeClusters in database \n')
                    except ValueError: pass
                    analysisfolder.database.getConnection()
                    analysisfolder.database.c.execute("alter table barcodeClusters add column constructTypes string")
                    analysisfolder.database.c.execute("alter table barcodeClusters add column readPairsInBamFile integer")
                    analysisfolder.database.c.execute("alter table barcodeClusters add column mappedSEReads integer")
                    analysisfolder.database.c.execute("alter table barcodeClusters add column SEreadsPassMappingQualityFilter integer")
                    analysisfolder.database.c.execute("alter table barcodeClusters add column goodReadPairs integer")
                    analysisfolder.database.c.execute("alter table barcodeClusters add column duplicateReadPairs integer")
                    analysisfolder.database.c.execute("alter table barcodeClusters add column goodReadPairPositions string")
                    analysisfolder.database.c.execute("alter table barcodeClusters add column htmlTable string")
                    analysisfolder.database.c.execute("alter table barcodeClusters add column analyzed BOOLEAN")
                if 'targetInfo' not in columnNames:
                    analysisfolder.database.c.execute("alter table barcodeClusters add column targetInfo string")
                if 'individual_ID_dictionary' not in columnNames:
                    analysisfolder.database.c.execute("alter table barcodeClusters add column individual_ID_dictionary string")
                if 'hetrozygous_positions' not in columnNames:
                    analysisfolder.database.c.execute("alter table barcodeClusters add column hetrozygous_positions int")
                if 'high_quality_cluster' not in columnNames:
                    analysisfolder.database.c.execute("alter table barcodeClusters add column high_quality_cluster BOOLEAN")                

                # update the values for all the clusters in the chunk
                analysisfolder.database.c.executemany('UPDATE barcodeClusters SET annotations=?, constructTypes=?,readPairsInBamFile=?, mappedSEReads=?, SEreadsPassMappingQualityFilter=?, goodReadPairs=?, duplicateReadPairs=?, goodReadPairPositions=?, targetInfo=?,individual_ID_dictionary=?, htmlTable=?, analyzed=?,hetrozygous_positions=?, high_quality_cluster=? WHERE clusterId=?', chunk)
                
                # commit the data and close the connection
                analysisfolder.database.commitAndClose()
                analysisfolder.database.writeInProgress.value = False
                
                # set flag to true
                updated = True
        
                print 'updated ',len(chunk),'clusters', updated,'(',','.join([str(x[-1]) for x in chunk]),')'
                time.sleep(1)
                
            except sqlite3.OperationalError as err:
                import sys
                msg = 'WARNING: Waiting for database connection! (retry in 1s, '+str(err)+').\n'
                sys.stderr.write(msg)
                #analysisfolder.logfile.write(msg)
                time.sleep(1) # will be executed if database is blocked

if __name__ == "__main__": main()