#! /usr/bin/env python

import sys

def main():
    
    #
    # Imports
    #
    import metadata
    import time
    
    #
    # check input and get commandline args
    #
    try:
	r1filename = sys.argv[1]
	r2filename = sys.argv[2]
	analysisfolder = metadata.AnalysisFolder(sys.argv[3])
    except IndexError: sys.stderr.write('please supply an outfile and infiles on format:\nwfaChecker <r1> <r2> <analysis-output-folder>\n');sys.exit()
    
    #
    # check analysis folder
    #
    if not analysisfolder.checkIntegrity() == 'PASS':
	if analysisfolder.checkIntegrity() == 'FAIL: The folder does not exist.': analysisfolder.create()
	else:
	    print analysisfolder.checkIntegrity()+'\nERROR: Now exiting'
    
    #
    # create a logfile
    #
    logfile = open(analysisfolder.logpath+'/'+time.strftime("%y%m%d-%H:%M:%S",time.localtime())+'_wfaChecker','w',1)
    logfile.write('cmd: '+' '.join(sys.argv)+'\n')
    analysisfolder.logfile = logfile
    
    #
    # add files to database
    #
    analysisfolder.database.addFastqs(r1filename, r2filename,logfile=logfile)
    reader = FastqReader(analysisfolder,logfile=logfile)
    reader.runParallel()
    
    logfile.write('wfaChecker FINISHED\n')

def foreachRead(readpair):
    readpair.isIlluminaAdapter()
    readpair.identifyDirection()
    readpair.makeColoredOut()
    readpair.fixInsert()
    readpair.matchdbs()
    return readpair

class FastqReader(object):
    """ reads fastq files (read one and two) and identifies the coordinates for the Barcode sequence etc and adds the data to the database """
    
    def __init__(self,analysisfolder,logfile=None):
	
	self.analysisfolder = analysisfolder
	self.logfile = logfile
	self.resultsummary=Summary(analysisfolder)

	#
	# Setting initial values
	#
	self.filesAdded = {}
	self.readsToAdd = []
	self.currentLocation = 1
	self.appendChunkSize = 500000
        self.getFilenames()
        self.totalReadcount = 0
        self.currentRead = 0
	self.grandTotal = 0
	self.minR1ReadLength = 10000000000
	self.minR2ReadLength = 10000000000

	#
	# get the grand total readcount in all files to be added to the database
	#
	for filePairId, readcount, fastq1, fastq2 in self.infiles: self.grandTotal += readcount
	if self.logfile: logfile.write('Going to add a self.grandTotal of '+str(self.grandTotal)+' reads to the database.\n')

	import misc
	self.progress = misc.Progress(self.grandTotal, logfile=self.logfile, unit='reads-post-processed', mem=True)
	
	#
	# open outfiles for inserts
	#
	self.analysisfolder.fastq_outfile1 = open(self.analysisfolder.fastq_outfile1,'w')
	self.analysisfolder.fastq_outfile2 = open(self.analysisfolder.fastq_outfile2,'w')
	self.analysisfolder.fastq_outfile3 = open(self.analysisfolder.fastq_outfile3,'w')
	self.analysisfolder.coloredReadMasking = open(self.analysisfolder.coloredReadMasking,'w')

    def getFilenames(self,):
        self.infiles = self.analysisfolder.database.getFastqs()

    def readPairGenerator(self,):

	#
	# imports
	#
	import gzip
	import misc
	import seqdata

        #
        # Loop through infiles
        #
	readfromdiskProgress = misc.Progress(self.grandTotal, logfile=self.logfile, unit='reads-read-from-disk', mem=True)
	with readfromdiskProgress:
	    for filePairId, readcount, fastq1, fastq2 in self.infiles:
		if self.logfile: self.logfile.write(str(self.currentRead)+' read pairs read from infiles, now starting to read from '+fastq1+'.\n')
		self.totalReadcount += readcount
		
		#
		# Open the files
		#
		if fastq1.split('.')[-1] in ['gz','gzip']: file1 = gzip.open(fastq1)
		else: file1 = open(fastq1,'r')
		if fastq2.split('.')[-1] in ['gz','gzip']: file2 = gzip.open(fastq2)
		else: file2 = open(fastq2,'r')
		
		while 'NOT EOFError':
		    try:
			header1 = file1.readline().rstrip()
			header2 = file2.readline().rstrip()
			sequence1 = file1.readline().rstrip()
			sequence2 = file2.readline().rstrip()
			trash = file1.readline().rstrip()
			trash = file2.readline().rstrip()
			qual1 = file1.readline().rstrip()
			qual2 = file2.readline().rstrip()
			if not header1: break
			self.currentRead += 1
			# data base has following info:
			#    (id,header,sequence1,sequence2,quality1,quality2,barcodeSequence,clusterId,annotation,fromFastq)
			barcodeSequence = None
			clusterId = None
			annotations = {}
			pair = seqdata.ReadPair(self.currentRead, header1, header2, sequence1, sequence2, qual1, qual2,barcodeSequence,clusterId,annotations, filePairId)#fastq1)
			readfromdiskProgress.update()
			yield pair#self.currentRead, header1, header2, sequence1, sequence2, qual1, qual2, fastq1
		    except EOFError: break
		assert self.totalReadcount == self.currentRead, 'Error while reading infiles: Read count after file '+fastq1+' is '+str(self.currentRead)+' should theoretically be '+str(self.totalReadcount)+'.\n'
		if self.logfile: self.logfile.write('Reached the end of '+fastq1+'.\n')
	if self.logfile: self.logfile.write(str(self.grandTotal)+' read pairs read from infiles.\n')
	self.analysisfolder.results.setResult('totalReadCount',self.grandTotal)

    def foreachProcessedPair(self, pair):
	#
	# append to chunk
	#
	self.readsToAdd.append(pair.databaseTuple)

	#
	# check read pair file origin
	#
	if pair.fileOrigin not in self.filesAdded:
	    if self.logfile: self.logfile.write('Starting post process of read pair #'+str(pair.id)+' (from file '+str(pair.fileOrigin)+').\n')
	    self.filesAdded[pair.fileOrigin] = True

	#
	# add chunk to db
	#
	if len(self.readsToAdd) >= self.appendChunkSize: self.chunkToDb()
	
	#
	# check read length
	#
	self.minR1ReadLength = min([self.minR1ReadLength,len(pair.r1Seq)])
	self.minR2ReadLength = min([self.minR2ReadLength,len(pair.r2Seq)])
	
	#
	# add to results summary
	#
	self.resultsummary.add(pair)
	
	#
	# write to files
	#
        self.analysisfolder.coloredReadMasking.write( pair.outputSeq + '\n')
	if pair.insert and pair.dbs and pair.dbsmatch:
	    if pair.insert[0] and len(pair.insert[0]) >= 50 and pair.insert[1] and len(pair.insert[1]) >= 50:
		self.analysisfolder.fastq_outfile1.write( str(pair.r1Header)+' '+str(pair.dbs)+'\n'+str(pair.insert[0])+'\n+\n'+str(pair.insert[2])+'\n')
		self.analysisfolder.fastq_outfile2.write( str(pair.r2Header)+' '+str(pair.dbs)+'\n'+str(pair.insert[1])+'\n+\n'+str(pair.insert[3])+'\n')
	    elif pair.insert[0] and len(pair.insert[0]) >= 50:
		self.analysisfolder.fastq_outfile3.write( str(pair.r1Header)+' '+str(pair.dbs)+'\n'+str(pair.insert[0])+'\n+\n'+str(pair.insert[2])+'\n')
	    elif pair.insert[1] and len(pair.insert[1]) >= 50:
		self.analysisfolder.fastq_outfile3.write( str(pair.r2Header)+' '+str(pair.dbs)+'\n'+str(pair.insert[1])+'\n+\n'+str(pair.insert[3])+'\n')
	    else: pass#print 'HERE',pair
	
	
	self.progress.update()
	
	return 0

    def chunkToDb(self, ):
	#
	# Imports
	#
	import time

	#chunkStartTime = time.time()
	chunkStartTime = time.time()
	#SEAseqPipeLine.logfile.write('Appending reads '+str(self.currentLocation)+'-'+str(self.currentLocation+self.appendChunkSize-1)+' to db.\n')
	if self.logfile: self.logfile.write('Staring to append reads '+str(self.currentLocation)+'-'+str(self.currentLocation+len(self.readsToAdd)-1)+' to db.\n')

	#SEAseqPipeLine.database.addReads(self.readsToAdd)
	self.analysisfolder.database.addReads(self.readsToAdd)

	#chunkTime = time.time()-chunkStartTime
	chunkTime = time.time()-chunkStartTime

	#SEAseqPipeLine.logfile.write('Reads '+str(self.currentLocation)+'-'+str(self.currentLocation+self.appendChunkSize-1)+' done after '+str(int(round(chunkTime,0)/60))+' minutes and '+str(round(chunkTime,0)%60)+' seconds.\n')
	if self.logfile: self.logfile.write('Reads '+str(self.currentLocation)+'-'+str(self.currentLocation+len(self.readsToAdd)-1)+' appended to db after '+str(int(round(chunkTime,0)/60))+' minutes and '+str(round(chunkTime,0)%60)+' seconds.\n')

	self.currentLocation += len(self.readsToAdd) #self.appendChunkSize

	self.readsToAdd = []
	
	return 0

    def run(self,):
	
	#
	# Write log message
	#
	if self.logfile: self.logfile.write('Adding reads to database table.\n')
	
	#
	# Parse through files and add to database in chunks of "self.appendChunkSize" reads
	#
	with self.progress:
	    for pair in self.readPairGenerator():
		#
		# foreach read do this before adding to db, this part could be done in parallel
		#
		pair = foreachRead(pair)
		
		#
		# post procesing for each read, preparing and adding to db
		#
		self.foreachProcessedPair(pair)


	#
	# add final chunk to db
	#
	if self.readsToAdd: self.chunkToDb()
	
	#
	# Done write to log and return
	#
	if self.logfile: logfile.write('Files '+', '.join([key for key in self.filesAdded.keys()])+' added sucesfully to the database.\n')
	
	return 0

    def runParallel(self,):
	
	#
	# imports
	#
	import multiprocessing
	
	#
	# drop old data and create table for new data
	#
	if self.logfile: self.logfile.write('Create reads table (and drop old one if needed) ...\n')
	self.analysisfolder.database.getConnection()
	self.analysisfolder.database.c.execute("DROP TABLE IF EXISTS reads")
	self.analysisfolder.database.c.execute('''CREATE TABLE reads (id,header,sequence1,sequence2,quality1,quality2,handleCoordinates,clusterId,annotation,fromFastq,PRIMARY KEY (id))''')
	if self.logfile: self.logfile.write('commiting changes to database.\n')
        self.analysisfolder.database.commitAndClose()
	
	#
	# Write log message
	#
	if self.logfile: self.logfile.write('Adding reads to database table (working in parallel).\n')
	
	poolOfProcesses = multiprocessing.Pool(self.analysisfolder.settings.parallelProcesses-1,maxtasksperchild=100000000)
	self.parallelResults = poolOfProcesses.imap_unordered(foreachRead,self.readPairGenerator(),chunksize=1000)
	
	#
	# Parse through files and add to database in chunks of "self.appendChunkSize" reads
	#
	adapterCount = 0
	with self.progress:
	    for pair in self.parallelResults:	    
		#
		# post procesing for each read, preparing and adding to db
		#
		self.foreachProcessedPair(pair)
		if 'Read1IsIlluminaAdapter' in pair.annotations or 'Read2IsIlluminaAdapter' in pair.annotations: adapterCount += 1
	self.analysisfolder.results.setResult('readPairsAreIlluminaAdapters',adapterCount)

	#
	# add final chunk to db
	#
	if self.readsToAdd: self.chunkToDb()
	poolOfProcesses.close()
	poolOfProcesses.join()
	
	#
	# Done write to log and return
	#
	if self.logfile: self.logfile.write('Files '+', '.join([str(key) for key in self.filesAdded.keys()])+' added sucesfully to the database.\n')
	if self.logfile: self.logfile.write('Saving totalReadCount etc to results table in database.\n')
	self.analysisfolder.results.setResult('minR1readLength',self.minR1ReadLength)
	self.analysisfolder.results.setResult('minR2readLength',self.minR2ReadLength)
	self.analysisfolder.results.saveToDb()
	self.resultsummary.printsummary()
	
	return 0

class Summary():
    
    def __init__(self, analysisfolder, logfile=None):
        self.totalReads = 0
        self.directions = {}
        self.constructType = {}
        self.dbsmatches = {}
	self.analysisfolder = analysisfolder
	self.logfile = logfile

    def add(self, readpair):
        self.totalReads += 1
        try: self.directions[readpair.direction] += 1
        except KeyError: self.directions[readpair.direction] = 1
        try: self.constructType[readpair.construct] += 1
        except KeyError: self.constructType[readpair.construct] = 1
        try: self.dbsmatches[readpair.dbsmatch] += 1
        except KeyError: self.dbsmatches[readpair.dbsmatch] = 1

    def printsummary(self):

	import misc

	print '\n##### SUMMARY #####'

        print self.totalReads

        print 'Directions:'
	self.analysisfolder.logfile.write ('Directions:\n')
        for direction,count in self.directions.iteritems():
            string = str(direction) +'\t'+str(misc.percentage(count,self.totalReads))
	    print string
	    self.analysisfolder.logfile.write (string+'\n')
        print ''
	self.analysisfolder.logfile.write ('\n')

        print 'CunstructType:'
	self.analysisfolder.logfile.write ('CunstructType:'+'\n')
        for constructtype,count in self.constructType.iteritems():
            string = str(constructtype) +'\t'+str(misc.percentage(count,self.totalReads))
	    print string
	    self.analysisfolder.logfile.write (string+'\n')
        print ''
	self.analysisfolder.logfile.write ('\n')

        print 'DBSmatch:'
	self.analysisfolder.logfile.write ('DBSmatch:'+'\n')
        for match,count in self.dbsmatches.iteritems():
            if match != None: string= ' '+str(match) +'\t'+str(misc.percentage(count,self.totalReads-self.dbsmatches[None]))+' (% of total-None)'
            else: string= ' '+str(match) +'\t'+str(misc.percentage(count,self.totalReads))+' (% of total)'
	    print string
	    self.analysisfolder.logfile.write (string+'\n')
        print ''
	self.analysisfolder.logfile.write ('\n')

	self.analysisfolder.results.setResult('readsWithDbsPatternMatch',self.dbsmatches)
	self.analysisfolder.results.setResult('constructTypes',self.constructType)
	self.analysisfolder.results.saveToDb()

if __name__ == "__main__": main()